{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b606520d",
   "metadata": {},
   "source": [
    "# Lasso vs Ridge Regression:  Numerical Analysis Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a3a01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using LinearAlgebra\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3d701",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb5fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_data_temp = CSV.File(\"median_housing_cost_data.tsv\") |> Tables.matrix\n",
    "matrix_data = matrix_data_temp[:,2:9];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221afc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_target_temp = CSV.File(\"housing_cost_targets.tsv\") |> Tables.matrix\n",
    "matrix_target = matrix_target_temp[:,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385d9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split (test proportion of 20%) - test has 4128 samples, train has 16512 samples\n",
    "\n",
    "matrix_data_train = matrix_data[1:16512, :];\n",
    "matrix_data_test = matrix_data[16513:20640, :];\n",
    "\n",
    "matrix_target_train = matrix_target[1:16512, :];\n",
    "matrix_target_test = matrix_target[16513:20640, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ab5f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data (min max normalize)\n",
    "#comment this out and run cell above if we wish to do it without normalizing\n",
    "\n",
    "for i in 1:size(matrix_data_train)[2]\n",
    "    \n",
    "    matrix_data_train[:,i] = (matrix_data_train[:,i] .- findmin(matrix_data_train[:,i])[1]) ./ (findmax(matrix_data_train[:,i])[1] - findmin(matrix_data_train[:,i])[1]);\n",
    "    matrix_data_test[:,i] = (matrix_data_test[:,i] .- findmin(matrix_data_test[:,i])[1]) ./ (findmax(matrix_data_test[:,i])[1] - findmin(matrix_data_test[:,i])[1]);\n",
    "\n",
    "end\n",
    "\n",
    "matrix_target_train = (matrix_target_train .- findmin(matrix_target_train)[1]) ./ (findmax(matrix_target_train)[1] - findmin(matrix_target_train)[1]);\n",
    "matrix_target_test = (matrix_target_test .- findmin(matrix_target_test)[1]) ./ (findmax(matrix_target_test)[1] - findmin(matrix_target_test)[1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac48a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75200bd4",
   "metadata": {},
   "source": [
    "## Motivating factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdef5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# talk about median household costs and how maximizing posterior probabilities to estimate cost is goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6742e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addc8dd1",
   "metadata": {},
   "source": [
    "## Regression tasks derived from maximizing posterior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5fabd1",
   "metadata": {},
   "source": [
    "Here we will derive from the Bayesian approach of maximizing w on a posterior probability the corresponding regression task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e17c8",
   "metadata": {},
   "source": [
    "For We derive the ridge regression task from the following.\n",
    "We have an observed data likelihood: $P(t|w) \\approx N(t|y, 1)$  \n",
    "and a prior distribution: $P(w|\\lambda) \\approx N(w|0,\\frac{1}{\\lambda})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8facc16",
   "metadata": {},
   "source": [
    "We wish to maximize the posterior probability: $P(w|t) = P(t|w) P(w|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5dcf7",
   "metadata": {},
   "source": [
    "$argmax_{w}(P(w|t)) = argmax_{w}(P(t|w) P(w|\\lambda))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34019e59",
   "metadata": {},
   "source": [
    "$= argmax_{w}(\\prod\\limits_{i=1}^{N}P(t_i|w)\\prod\\limits_{j=0}^{M}P(w_j|0,\\lambda))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50e751",
   "metadata": {},
   "source": [
    "$= argmax_{w}(\\prod\\limits_{i=1}^{N}N(t|y_i,1)\\prod\\limits_{j=0}^{M}N(w_j|0,\\frac{1}{\\lambda}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311ba83",
   "metadata": {},
   "source": [
    "$\\propto argmax_{w}(\\sum\\limits_{i=1}^{N}ln(N(t_i|y_i,1)) + \\sum\\limits_{j=0}^{M}ln(N(w_j|0,\\lambda)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e037b",
   "metadata": {},
   "source": [
    "$= argmax_{w}(\\sum\\limits_{i=1}^{N}ln(e^{\\frac{-1}{2}(t_i - y_i)^{T}1(t_i - y_i)}) + \\sum\\limits_{j=0}^{M}ln(e^{\\frac{-1}{2}(w - 0)^{T}\\lambda(w - 0)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c071c1",
   "metadata": {},
   "source": [
    "$= argmin_{w}(-(\\frac{-1}{2}\\sum\\limits_{i=1}^{N}(t_i - y_i)^2 - \\frac{\\lambda}{2} \\sum\\limits_{j=0}^{M} w_j^2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b8dec",
   "metadata": {},
   "source": [
    "$= argmin_{w}(\\frac{1}{2}\\sum\\limits_{i=1}^{N}(t_i - y_i)^2 + \\frac{\\lambda}{2} \\sum\\limits_{j=0}^{M} w_j^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab070a4c",
   "metadata": {},
   "source": [
    "As we can see, we have successfully derived the ridge (L2) regression formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1668edf",
   "metadata": {},
   "source": [
    "We derive the lasso regression task from the following.\n",
    "We have an observed data likelihood: $P(t|w) \\approx N(t|y, 1)$  \n",
    "and a prior distribution: $P(w|\\lambda) \\approx Poisson(w|\\frac{1}{\\lambda})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0dfde8",
   "metadata": {},
   "source": [
    "We wish to maximize the posterior probability: $P(w|t) = P(t|w) P(w|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95460b51",
   "metadata": {},
   "source": [
    "$argmax_w(P(w|t)) = argmax_w(P(t|w) P(w|\\lambda))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74776d19",
   "metadata": {},
   "source": [
    "$=argmax_w(\\prod\\limits_{i=1}^{N}N(t_i|y_i,1) \\prod\\limits_{j=0}^{M}Poisson(w|\\frac{1}{\\lambda}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d750fd6",
   "metadata": {},
   "source": [
    "$\\propto argmax_w(\\sum\\limits_{i=1}^{N}ln(N(t_i|y_i, 1)) + \\sum\\limits_{j=0}^{M}ln(Poisson(\\frac{1}{\\lambda})))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c02ff",
   "metadata": {},
   "source": [
    "$\\propto argmax_w(\\sum\\limits_{i=1}^{N}ln(e^{\\frac{-1}{2}(t_i-y_i)^T1(t_i-y_i)}) + \\sum\\limits_{j=0}^{M}ln(e^{-w_j\\lambda}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6983237",
   "metadata": {},
   "source": [
    "$=argmin_w(-(\\frac{-1}{2}\\sum\\limits_{i=1}^{N}(t_i-y_i)^2 - \\lambda\\sum\\limits_{j=0}^{M}|w_j| ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0ce31",
   "metadata": {},
   "source": [
    "$=argmin_w(\\frac{1}{2}\\sum\\limits_{i=1}^{N}(t_i-y_i)^2 + \\lambda\\sum\\limits_{j=0}^{M}|w_j| )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e68356",
   "metadata": {},
   "source": [
    "As we can see, we have successfully derived the lasso (L1) regression formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11faaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80429285",
   "metadata": {},
   "source": [
    "## Solutions to regression tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b35da",
   "metadata": {},
   "source": [
    "Now that we have derived the formulation for ridge regression, let's derive a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab47d6d",
   "metadata": {},
   "source": [
    "$argmin_w(\\frac{1}{2}\\sum\\limits_{i=1}^{N}(t_i-y_i)^2 + \\lambda\\sum\\limits_{j=0}^{M}w_j^2 )$ , inherently implies we should differentiate argument to be minimized with respect to the argument we are trying to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c59b6",
   "metadata": {},
   "source": [
    "If we define: $J(w) = \\frac{1}{2}\\sum\\limits_{i=1}^{N}(t_i-y_i)^2 + \\frac{\\lambda}{2}\\sum\\limits_{j=0}^{M}w_j^2$ , then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996fa74",
   "metadata": {},
   "source": [
    "$J(w) = \\frac{1}{2}(t-Xw)^T(t-Xw) + \\frac{\\lambda}{2}w^Tw$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf125fe2",
   "metadata": {},
   "source": [
    "$ = \\frac{1}{2} (t^Tt - t^TXw - w^TX^Tt + w^TX^TXw) + \\frac{\\lambda}{2}w^Tw$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74deee07",
   "metadata": {},
   "source": [
    "We next differentiate $J(w)$ with respect to $w$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb706101",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(w)}{\\partial w} = \\frac{1}{2}(0 - 2t^TX + 2w^TX^TX) + \\frac{\\lambda}{2}2w^TI$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6ff45",
   "metadata": {},
   "source": [
    "$ = -t^TX + w^TX^TX + \\lambda w^TI$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298c0f1",
   "metadata": {},
   "source": [
    "Since, $\\frac{\\partial J(w)}{\\partial w} = 0$, we can thus rearrange terms to get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec876a2",
   "metadata": {},
   "source": [
    "$t^TX = w^TX^TX + \\lambda w^TI$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabcbaa",
   "metadata": {},
   "source": [
    "$(t^TX)^T = (w^TX^TX)^T + (\\lambda w^TI)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375cdaa",
   "metadata": {},
   "source": [
    "$X^Tt = X^TXw + \\lambda wI$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b9574",
   "metadata": {},
   "source": [
    "$X^Tt = w(X^TX + \\lambda I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9be62",
   "metadata": {},
   "source": [
    "so we thus get as our final solution:  \n",
    "$w = (X^TX + \\lambda I)^{-1}X^Tt$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1dd3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0acb3b",
   "metadata": {},
   "source": [
    "Now that we have a derived a formulation for lasso regression, let's find a solution. We will quickly realize in attempting to derive it, that there is no simple closed form solution to lasso regression. There exists a complex closed form solution that uses proximity functions, but this is not easy to implement in code. Thus, we will solve a solution to lasso regression numerically with a numerical method, more specifically, gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0c4c6",
   "metadata": {},
   "source": [
    "If we define: $J(w) = \\frac{1}{2}\\sum\\limits_{i=1}^{N}(t_i-y_i)^2 + \\lambda\\sum\\limits_{j=0}^{M}|w_j|$ , then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c92431",
   "metadata": {},
   "source": [
    "$J(w) = \\frac{1}{2}(t-Xw)^T(t-Xw) + \\lambda |w|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebc111",
   "metadata": {},
   "source": [
    "$ = \\frac{1}{2} (t^Tt - t^TXw - w^TX^Tt + w^TX^TXw) + \\lambda |w| $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378d8e6",
   "metadata": {},
   "source": [
    "Thus we have it that: $\\frac{\\partial J(w)}{\\partial w} = 0$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f160a30",
   "metadata": {},
   "source": [
    "and therefore, $-t^TX + w^TX^TX + \\alpha \\lambda$, such that, $\\alpha = sign(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60089650",
   "metadata": {},
   "source": [
    "$ => -(X^T(t-Xw + \\alpha \\lambda))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c99d8",
   "metadata": {},
   "source": [
    "From here, we can see that we cannot solve for w easily with a closed form solution. We thus will use this derivative and express it in the form of the gradient of MSE as the update term in gradient descent - a numerical method used to approximate and converge upon the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07afc0",
   "metadata": {},
   "source": [
    "in gradient descent we have it that: $w^{(t+1)} <= w^{(t)} - \\eta \\nabla_w MSE(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d01fd",
   "metadata": {},
   "source": [
    "To express what we have thus far into the gradient of MSE term, all me must do is multiply by 2 and divide by the number of samples since it is \"mean\" squared error: $\\nabla_w MSE(w) = -\\frac{2}{m}(X^T(t-Xw + b + \\alpha \\lambda))$, where b is the bias that comes from the gradient descent, which we define as: $-\\frac{2}{m}\\sum\\limits_{i=1}^{N}(t_i-Xw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046b474",
   "metadata": {},
   "source": [
    "So in gradient descent we have it that:  \n",
    "$w^{(t+1)} <= w^{(w)} - \\eta (-\\frac{2}{m}(X^T(t-Xw + b + \\alpha \\lambda)))$  \n",
    "$b^{(t+1)} <= b^{(t)} - \\eta (-\\frac{2}{m}\\sum\\limits_{i=1}^{N}(t_i-Xw)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1f349",
   "metadata": {},
   "source": [
    "These are the update equations we will use to iterate towards the solution for the parameters for lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20571f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2bf9dc",
   "metadata": {},
   "source": [
    "## Code solutions for regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64bab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_ridge (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear_regression_ridge(X,y,lambda)\n",
    "    \n",
    "    s = size(X)[2]\n",
    "    Im =1* Matrix(I, s, s)\n",
    "    \n",
    "    w = inv(transpose(X)*X + lambda*Im) * transpose(X) * y\n",
    "    \n",
    "    pred = X * w\n",
    "    \n",
    "    err = y - pred\n",
    "    \n",
    "    return w    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3810ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_lasso_GD (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear_regression_lasso_GD(X, y, lambda)\n",
    "    \n",
    "    learning_rate = .025\n",
    "    iterations = 8000\n",
    "    l1_penalty = lambda\n",
    "    n = size(X)[2] # feature number\n",
    "    m = size(X)[1] #sample number\n",
    "    w = zeros(n) # shape of the params (feature #)\n",
    "    b = 0\n",
    "    \n",
    "    for i in 1:iterations        \n",
    "        y_pred = zeros(m)\n",
    "        for k in 1:m\n",
    "            y_pred[k] = dot(X[k,:], w) + b\n",
    "        end\n",
    "        \n",
    "        #calculate gradients\n",
    "        dW = zeros(n) # shape of the params (feature #)\n",
    "        for j in 1:n\n",
    "            if w[j] > 0  \n",
    "                dW[j] = ( -1 * (2 * (dot(X[:,j], y - y_pred) ) ) + l1_penalty) ./ m\n",
    "            else\n",
    "                dW[j] = ( -1 * (2 * (dot(X[:,j], y - y_pred) ) ) - l1_penalty) ./ m\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        db = - 2 * sum(y - y_pred) ./ m\n",
    "        \n",
    "        w = w - learning_rate*dW\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return w, b\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a01c4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_test (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for performing linear regression on some data X with provided coefficients w\n",
    "function linear_regression_test(X,w)\n",
    "    \n",
    "    pred = X * w\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8568c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_test_GD (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for performing linear regression on some data X with provided coefficients w for gradient descent regression\n",
    "function linear_regression_test_GD(X,w,b)\n",
    "    \n",
    "    m = size(X)[1]\n",
    "    pred = zeros(m)\n",
    "    for k in 1:m\n",
    "        pred[k] = dot(X[k,:], w) + b\n",
    "    end\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bb10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf36886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff64eaf9",
   "metadata": {},
   "source": [
    "## Analysis of algorithms: conditioning, complexities, and flop counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4e1b7",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1347b3",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e3ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function checks the conditioning of the Ridge Regression\n",
    "# by checking each individual operation to see if it is well conditioned.\n",
    "# If each individual operation of the regression is well conditioned, \n",
    "# then the entire algorithm is well conditioned. If a part of the \n",
    "# regression is ill conditioned then the algorithm is not optimal.\n",
    "function ridge_conditioning(X,y,lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c94f99",
   "metadata": {},
   "source": [
    "## Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbb297",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89db7b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_TC (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression Function Decomposed to show computations:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function ridge_TC(X,y,lambda)\n",
    "    s  = size(X)[2]                          # O(1)      [CHANGED [1] to [2]... Correct (grab m or n)?]\n",
    "    Im = 1 * Matrix(I, s, s)                 # O(n^2)    [Constructing (n x n) I Matrix]\n",
    "    xT = transpose(X)                        # O(1)      [(m x n)] => (n x m)\n",
    "    xT_X = xT * X                            # O(m^2*n)  [(n x m) * (m x n)] => (n x n)\n",
    "    l_Im = lambda * Im                       # O(n^2)    [Scalar multiplication of (n x n) matrix]\n",
    "    xT_lambda = l_Im + xT_X                  # O(n^2)    [Scalar addition of (n x n) matrix]\n",
    "    inverse = inv(xT_lambda)                 # O(n^3)    [Inverse (Gauss Elimination) with matrix of size (n x n)]\n",
    "    w = inverse * xT                         # O(n^2*m)  [(n x n) * (n x m)] => (n x m)\n",
    "    w *= y                                   # O(m*n)    [(n x m) * (m x 1)] => (n x 1)\n",
    "    pred = X * w                             # O(m*n)    [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                           # O(m)      [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w, pred, err                      # O(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cba48c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Complexities Added\n",
    "# O(n^3) + O(n^2*m) + O(m^2*n) + O(n^2) + O(n^2) + O(n^2) + O(m*n) + O(m*n) + O(m) + O(1) + O(1) + O(1)\n",
    "\n",
    "# Final Time Complexity: \n",
    "# O(n^3) + O(n^2*m) + O(m^2*n)\n",
    "\n",
    "# Note: If m >> n (Data points >> Features), time complexity can be reduced to O(m^2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4bb353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Dimensions\n",
    "# --------------------------------\n",
    "# lambda = constant\n",
    "# (n x n) = xT_X, inverse, Im, l_Im xT_lambda\n",
    "# (n x m) = xT\n",
    "# (n x 1) = w\n",
    "# (m x n) = X\n",
    "# (m x 1) = y, pred, err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e920e",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3928933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lasso_TC (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso Regression Function Decomposed to show computations:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function lasso_TC(X,y,lambda)  \n",
    "    xT = transpose(X)              # O(1)        [(m x n)] => (n x m)\n",
    "    xT_X = xT * X                  # O(m^2*n)    [(n x m) * (m x n)] => (n x n)\n",
    "    xInv = inv(xT_X)               # O(n^3)      [Inverse (Gauss Elimination) with matrix of size n]\n",
    "    moore_pen = xInv * xT          # O(n^2*m)    [(n x n) * (n x m)] => (n x m)\n",
    "    w = moore_pen * y              # O(m*n)      [(n x m) * (m x 1)] => (n x 1)\n",
    "    w_lam = (abs.(w) .- lambda)    # O(n)        [Scalar subtraction of (n x 1) matrix]\n",
    "    w_lasso = sign.(w) .* w_lam    # O(n)        [Scalar multiplication of (n x 1) matrix]\n",
    "    pred = X * w_lasso             # O(m*n)      [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                 # O(m)        [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w_lasso, pred, err      # O(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11ec65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Dimensions\n",
    "# -------------------------\n",
    "# lambda = constant\n",
    "# (n x n) = xT_X\n",
    "# (n x m) = xT, moore_pen\n",
    "# (n x 1) = w, w_lam, w_lasso\n",
    "# (m x n) = X\n",
    "# (m x 1) = y, pred, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20bc7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Complexity References\n",
    "# ----------------------------------------\n",
    "# Size(X)       : O(1)\n",
    "# References    : [https://stackoverflow.com/questions/21614298/what-is-the-runtime-of-array-length, https://blog.finxter.com/python-list-length-whats-the-runtime-complexity-of-len/, ]\n",
    "\n",
    "# Transpose(X)  : O(1)    \n",
    "# References    : [https://www.mathworks.com/matlabcentral/answers/495668-what-s-the-transpose-complexity-big-o-in-matlab, https://stackoverflow.com/questions/61157101/in-julia-transpose-operator]\n",
    "\n",
    "# Inverse(X)    : Worst Case-O(n^3) (Gauss Elimination), Best Case O(n^2.373)\n",
    "# References    : [https://stackoverflow.com/questions/54890422/inv-versus-on-julia]\n",
    "\n",
    "# Matrix *      : (m x n) * (n * p) => O(n*m*p), O(n^3)-O(n^2.72...)\n",
    "# References    : [https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra]\n",
    "\n",
    "# Matrix -      : O(m*n)\n",
    "# References    : [https://www.geeksforgeeks.org/different-operation-matrices/]\n",
    "\n",
    "# Matrix(I,s,s) : O(n^2)\n",
    "# References    : []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90bbd3",
   "metadata": {},
   "source": [
    "## Space Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19129928",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cda0b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_SC (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual Ridge Algorithm\n",
    "function ridge_SC(X,y,lambda)\n",
    "    n = size(X)[2]                                          # O(1)    1 variable\n",
    "    Im = 1 * Matrix(I, n, n)                                # O(n^2)  (n x n) matrix\n",
    "    w = inv(transpose(X)*X + lambda*Im) * transpose(X) * y  # O(n)    (n x 1) matrix\n",
    "    pred = X * w                                            # O(m)    (m x 1) matrix\n",
    "    err = y - pred                                          # O(m)    (m x 1) matrix\n",
    "    \n",
    "    return w, pred, err                                     # O(1)    Return on Stack\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42feb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Space Complexity\n",
    "# O(n^2) + O(n) + O(m) + O(m) + O(1) + O(1)\n",
    "\n",
    "# O(n^2) + O(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4f694",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9c9ebf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lasso_SC (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual Ridge Algorithm\n",
    "function lasso_SC(X,y,lambda)\n",
    "    moore_pen = inv(transpose(X)*X) * transpose(X)    # O(m*n) (n x m) matrix\n",
    "    w = moore_pen * y                                 # O(n)   (n x 1) matrix\n",
    "    w_lasso = sign.(w) .* (abs.(w) .- lambda)         # O(n)   (n x 1) matrix\n",
    "    pred = X * w_lasso                                # O(m)   (m x 1) matrix\n",
    "    err = y - pred                                    # O(m)   (m x 1) matrix\n",
    "    \n",
    "    return w_lasso, pred, err                         # O(1)   Return on Stack\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a947473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Space Complexity\n",
    "# O(m*n) + O(n) + O(n) + O(m) + O(m) + O(1)\n",
    "\n",
    "# O(m*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e8c9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space Complexity References\n",
    "# -------------------------------\n",
    "# [Space Complexity Calculates Temp Vars?] https://www.studytonight.com/data-structures/space-complexity-of-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3e75f",
   "metadata": {},
   "source": [
    "## Flop Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b9cba",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7242e0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_FC (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression Function Decomposed:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function ridge_FC(X,y,lambda)\n",
    "    n  = size(X)[2]                          # 0\n",
    "    Im = 1 * Matrix(I, n, n)                 # n^2          [Constructing (n x n) I Matrix]\n",
    "    xT = transpose(X)                        # 0            [(m x n)] => (n x m)\n",
    "    xT_X = xT * X                            # m^2*n        [(n x m) * (m x n)] => (n x n)\n",
    "    l_Im = lambda * Im                       # n^2          [Scalar multiplication of (n x n) matrix]\n",
    "    xT_lambda = l_Im + xT_X                  # n^2          [Scalar addition of (n x n) matrix]\n",
    "    inverse = inv(xT_lambda)                 # n^3 (approx) [Gauss Elimination with matrix of size (n x n)]\n",
    "    w = inverse * xT                         # n^2*m        [(n x n) * (n x m)] => (n x m)\n",
    "    w *= y                                   # m*n          [(n x m) * (m x 1)] => (n x 1)\n",
    "    pred = X * w                             # m*n          [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                           # m            [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w, pred, err                      # 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ca6d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count\n",
    "# n^2 + (m^2*n) + n^2 + n^2 + n^3 + n^2*m + m*n + m*n + m\n",
    "# Total Flop Count = n^3 + (3+m)n^2 + n*m^2 + 2m*n + m\n",
    "\n",
    "# Note: In the worst case, Gaussian Elimination for n x n will take ((5/6)n^3+(3/2)n^2-(7/6)n) floating point operations.\n",
    "# Worst Case Flops = (5/6)n^3 + ((9/2)+m)n^2 + n*m^2 + ((5/6)m)n + m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4d409",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcb526fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lasso_TC (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression Function Decomposed:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function lasso_TC(X,y,lambda)  \n",
    "    xT = transpose(X)              # 0\n",
    "    xT_X = xT * X                  # m^2*n        [(n x m) * (m x n)] => (n x n)\n",
    "    xInv = inv(xT_X)               # n^3 (approx) [Gauss Elimination with matrix of size (n x n)]\n",
    "    moore_pen = xInv * xT          # n^2*m        [(n x n) * (n x m)] => (n x m)\n",
    "    w = moore_pen * y              # m*n          [(n x m) * (m x 1)] => (n x 1)\n",
    "    w_lam = (abs.(w) .- lambda)    # n            [Scalar subtraction of (n x 1) matrix]\n",
    "    w_lasso = sign.(w) .* w_lam    # n            [Scalar multiplication of (n x 1) matrix]\n",
    "    pred = X * w_lasso             # m*n          [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                 # m            [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w_lasso, pred, err      # 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ad50977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count\n",
    "# Flops = n^3 + n*m^2 + m*n^2 + 2m*n + 2n + m\n",
    "\n",
    "# Note: In the worst case, Gaussian Elimination for n x n will take ((5/6)n^3+(3/2)n^2-(7/6)n) floating point operations.\n",
    "# Worst Case Flops = (5/6)n^3 + n*m^2 + (3/2 + m)*n^2 + 2m*n + (5/6)n + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c2d4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count References:\n",
    "# Flops For nxn Gaussian Elimination: http://web.mit.edu/18.06/www/Fall15/Matrices.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e3ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71304cb4",
   "metadata": {},
   "source": [
    "## Perform regression tasks on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4a96f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r_squared (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function r_squared(targets, predictions)\n",
    "    \n",
    "    mean_target = mean(targets)\n",
    "    \n",
    "    ssr = 0\n",
    "    sst = 0\n",
    "    \n",
    "    for i in 1:size(targets)[1]\n",
    "        \n",
    "        ssr += (targets[i] - predictions[i])^2\n",
    "        sst += (targets[i] - mean_target)^2\n",
    "        \n",
    "    end\n",
    "    \n",
    "    r_sq = 1 - (ssr/sst)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d11a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c117f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [.000001, .00001, .0001, .001, .01, .05, .1, .5, 1.5, 5, 10, 12, 15, 18, 20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c637d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_l1_params = zeros(8)\n",
    "optimal_l2_params = zeros(8)\n",
    "\n",
    "optimal_l1_r_sq = -1\n",
    "optimal_l2_r_sq = -1\n",
    "\n",
    "optimal_l1_lam = -1000\n",
    "optimal_l2_lam = -1000\n",
    "\n",
    "for it in lambdas\n",
    "    \n",
    "    parameters_l2 = linear_regression_ridge(matrix_data_train,matrix_target_train, it);\n",
    "    parameters_l1, b_l1 = linear_regression_lasso_GD(matrix_data_train,matrix_target_train, it);\n",
    "    \n",
    "    predictions_l2 = linear_regression_test(matrix_data_test, parameters_l2);\n",
    "    predictions_l1 = linear_regression_test_GD(matrix_data_test, parameters_l1, b_l1);\n",
    "    predictions_l1 = reshape(predictions_l1, length(predictions_l1), 1);\n",
    "    \n",
    "    r_sq_l2 = r_squared(sort(matrix_target_test, dims=1), sort(predictions_l2, dims=1));\n",
    "    r_sq_l1 = r_squared(sort(matrix_target_test, dims=1), sort(predictions_l1, dims=1));\n",
    "    \n",
    "    if r_sq_l2 > optimal_l2_r_sq\n",
    "        optimal_l2_r_sq = r_sq_l2\n",
    "        optimal_l2_params = parameters_l2\n",
    "        optimal_l2_lam = it\n",
    "    end\n",
    "    \n",
    "    if r_sq_l1 > optimal_l1_r_sq\n",
    "        optimal_l1_r_sq = r_sq_l1\n",
    "        optimal_l1_params = parameters_l1\n",
    "        optimal_l1_lam = it\n",
    "    end\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a1025c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal lambda coefficient for l1 norm is:1.0e-5\n",
      "The optimal lambda coefficient for l2 norm is:5.0\n",
      "The optimal r^2 value for l1 norm is:0.9046658818867916\n",
      "The optimal r^2 value for l2 norm is:0.8722628864709415\n"
     ]
    }
   ],
   "source": [
    "println(\"The optimal lambda coefficient for l1 norm is:\", optimal_l1_lam)\n",
    "println(\"The optimal lambda coefficient for l2 norm is:\", optimal_l2_lam)\n",
    "println(\"The optimal r^2 value for l1 norm is:\", optimal_l1_r_sq)\n",
    "println(\"The optimal r^2 value for l2 norm is:\", optimal_l2_r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059a69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81a3836",
   "metadata": {},
   "source": [
    "## Evaluation of algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r^2 and qq plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3da3ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already have the r^2 values, possibly just recalculate them for the optimal params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ad7e6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a qq-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1914d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ab9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard code in the optimal regularization coefficients to function\n",
    "parameters_l2 = linear_regression_ridge(matrix_data_train,matrix_target_train, 5);\n",
    "parameters_l1, b_l1 = linear_regression_lasso_GD(matrix_data_train,matrix_target_train, .00001);\n",
    "    \n",
    "predictions_l2 = linear_regression_test(matrix_data_test, parameters_l2);\n",
    "predictions_l1 = linear_regression_test_GD(matrix_data_test, parameters_l1, b_l1);\n",
    "predictions_l1 = reshape(predictions_l1, length(predictions_l1), 1);\n",
    "    \n",
    "sorted_test_targets = sort(matrix_target_test, dims=1)\n",
    "sorted_l2_preds = sort(predictions_l2, dims=1)\n",
    "sorted_l1_preds = sort(predictions_l1, dims=1)\n",
    "r_sq_l2 = r_squared(sorted_test_targets, sorted_l2_preds);\n",
    "r_sq_l1 = r_squared(sorted_test_targets, sorted_l1_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0588fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: someone please do this, my julia won't let me add the Plots package. Can someone graph the QQ plots here plz.\n",
    "# it should concist of 2 qq plots. each should have a x=y line. \n",
    "# first qq plot should have sorted_l1_preds vs sorted_test_targets\n",
    "# second qq plot should have sorted_l2_preds vs sorted_test_targets\n",
    "# then under each qq plot just print(r_sq_l#) then it is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4495f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284bcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70126167",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80172b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb8561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78531dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600bcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
