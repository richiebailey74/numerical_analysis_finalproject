{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b606520d",
   "metadata": {},
   "source": [
    "# Numerical Analysis Final Project: Lasso vs Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f055eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5479c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Kingston\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MutableArithmetics â”€ v1.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Static â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v0.6.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Preferences â”€â”€â”€â”€â”€â”€â”€â”€ v1.2.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FiniteDiff â”€â”€â”€â”€â”€â”€â”€â”€â”€ v2.11.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v0.21.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IJulia â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v1.23.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Polynomials â”€â”€â”€â”€â”€â”€â”€â”€ v3.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NaNMath â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v0.3.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StaticArrays â”€â”€â”€â”€â”€â”€â”€ v1.4.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Compat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v3.43.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v1.2.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Conda â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v1.7.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InverseFunctions â”€â”€â”€ v0.1.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LogExpFunctions â”€â”€â”€â”€ v0.3.12\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v1.7.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Parsers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v2.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m VersionParsing â”€â”€â”€â”€â”€ v1.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JLLWrappers â”€â”€â”€â”€â”€â”€â”€â”€ v1.4.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArrayInterface â”€â”€â”€â”€â”€ v5.0.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRulesCore â”€â”€â”€â”€â”€ v1.14.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ v0.10.4\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      " \u001b[90m [336ed68f] \u001b[39m\u001b[93mâ†‘ CSV v0.10.0 â‡’ v0.10.4\u001b[39m\n",
      " \u001b[90m [7073ff75] \u001b[39m\u001b[93mâ†‘ IJulia v1.23.2 â‡’ v1.23.3\u001b[39m\n",
      " \u001b[90m [f27b6e38] \u001b[39m\u001b[93mâ†‘ Polynomials v2.0.24 â‡’ v3.0.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      " \u001b[90m [4fba245c] \u001b[39m\u001b[93mâ†‘ ArrayInterface v4.0.3 â‡’ v5.0.7\u001b[39m\n",
      " \u001b[90m [336ed68f] \u001b[39m\u001b[93mâ†‘ CSV v0.10.0 â‡’ v0.10.4\u001b[39m\n",
      " \u001b[90m [d360d2e6] \u001b[39m\u001b[93mâ†‘ ChainRulesCore v1.11.6 â‡’ v1.14.0\u001b[39m\n",
      " \u001b[90m [34da2185] \u001b[39m\u001b[93mâ†‘ Compat v3.41.0 â‡’ v3.43.0\u001b[39m\n",
      " \u001b[90m [8f4d0f93] \u001b[39m\u001b[93mâ†‘ Conda v1.6.0 â‡’ v1.7.0\u001b[39m\n",
      " \u001b[90m [e2ba6199] \u001b[39m\u001b[91m- ExprTools v0.1.8\u001b[39m\n",
      " \u001b[90m [6a86dc24] \u001b[39m\u001b[93mâ†‘ FiniteDiff v2.10.1 â‡’ v2.11.0\u001b[39m\n",
      " \u001b[90m [7073ff75] \u001b[39m\u001b[93mâ†‘ IJulia v1.23.2 â‡’ v1.23.3\u001b[39m\n",
      " \u001b[90m [d8418881] \u001b[39m\u001b[91m- Intervals v1.5.0\u001b[39m\n",
      " \u001b[90m [3587e190] \u001b[39m\u001b[93mâ†‘ InverseFunctions v0.1.2 â‡’ v0.1.3\u001b[39m\n",
      " \u001b[90m [692b3bcd] \u001b[39m\u001b[93mâ†‘ JLLWrappers v1.4.0 â‡’ v1.4.1\u001b[39m\n",
      " \u001b[90m [682c06a0] \u001b[39m\u001b[93mâ†‘ JSON v0.21.2 â‡’ v0.21.3\u001b[39m\n",
      " \u001b[90m [2ab3a3ac] \u001b[39m\u001b[93mâ†‘ LogExpFunctions v0.3.6 â‡’ v0.3.12\u001b[39m\n",
      " \u001b[90m [78c3b35d] \u001b[39m\u001b[91m- Mocking v0.7.3\u001b[39m\n",
      " \u001b[90m [d8a4904e] \u001b[39m\u001b[93mâ†‘ MutableArithmetics v0.3.2 â‡’ v1.0.0\u001b[39m\n",
      " \u001b[90m [77ba4419] \u001b[39m\u001b[93mâ†‘ NaNMath v0.3.6 â‡’ v0.3.7\u001b[39m\n",
      " \u001b[90m [69de0a69] \u001b[39m\u001b[93mâ†‘ Parsers v2.1.3 â‡’ v2.2.4\u001b[39m\n",
      " \u001b[90m [f27b6e38] \u001b[39m\u001b[93mâ†‘ Polynomials v2.0.24 â‡’ v3.0.0\u001b[39m\n",
      " \u001b[90m [21216c6a] \u001b[39m\u001b[93mâ†‘ Preferences v1.2.3 â‡’ v1.2.5\u001b[39m\n",
      " \u001b[90m [aedffcd0] \u001b[39m\u001b[93mâ†‘ Static v0.5.6 â‡’ v0.6.0\u001b[39m\n",
      " \u001b[90m [90137ffa] \u001b[39m\u001b[93mâ†‘ StaticArrays v1.3.3 â‡’ v1.4.3\u001b[39m\n",
      " \u001b[90m [82ae8749] \u001b[39m\u001b[93mâ†‘ StatsAPI v1.2.0 â‡’ v1.2.2\u001b[39m\n",
      " \u001b[90m [bd369af6] \u001b[39m\u001b[93mâ†‘ Tables v1.6.1 â‡’ v1.7.0\u001b[39m\n",
      " \u001b[90m [f269a46b] \u001b[39m\u001b[91m- TimeZones v1.7.1\u001b[39m\n",
      " \u001b[90m [81def892] \u001b[39m\u001b[93mâ†‘ VersionParsing v1.2.1 â‡’ v1.3.0\u001b[39m\n",
      " \u001b[90m [4af54fe1] \u001b[39m\u001b[91m- LazyArtifacts\u001b[39m\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Conda â”€â†’ `C:\\Users\\Kingston\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\6e47d11ea2776bc5627421d59cdcc1296c058071\\build.log`\n",
      "\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m IJulia â†’ `C:\\Users\\Kingston\\.julia\\scratchspaces\\44cfe95a-1eb2-52ea-b672-e2afdf69b78f\\98ab633acb0fe071b671f6c1785c46cd70bb86bd\\build.log`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mStatsAPI\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mPreferences\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mNaNMath\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mInverseFunctions\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mCompat\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mJLLWrappers\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mStatic\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mDistances\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mTables\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mOpenSSL_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mChainRulesCore\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFilePathsBase\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mDataStructures\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibmount_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mGraphite2_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mBzip2_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mMutableArithmetics\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mRecipesPipeline\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mlibsodium_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXau_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mlibfdk_aac_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mlibpng_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLAME_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mPixman_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mEarCut_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLERC_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mOgg_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mJpegTurbo_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXdmcp_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mx265_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mx264_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mStaticArrays\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mZstd_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mExpat_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLZO_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mOpus_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xtrans_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibiconv_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mWayland_protocols_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibffi_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mPCRE_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibgpg_error_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libpthread_stubs_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mOpenSpecFun_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibuuid_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFriBidi_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mArrayInterface\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mSortingAlgorithms\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mChangesOfVariables\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFreeType2_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mlibvorbis_jll\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mZeroMQ_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mDiffResults\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mContour\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibtiff_jll\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mPrettyTables\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39mPolynomials\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mStructArrays\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXML2_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibgcrypt_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFontconfig_jll\u001b[39m\n",
      "\u001b[33m  âœ“ \u001b[39m\u001b[90mZMQ\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLogExpFunctions\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFiniteDiff\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mWayland_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mGettext_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXSLT_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39mCSV\n",
      "\u001b[33m  âœ“ \u001b[39mPandas\n",
      "\u001b[33m  âœ“ \u001b[39mIJulia\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mStatsBase\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mGlib_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libxcb_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xcb_util_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libX11_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xcb_util_image_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xcb_util_renderutil_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xcb_util_keysyms_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xcb_util_wm_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mSpecialFunctions\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mGeometryBasics\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXrender_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXext_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libxkbfile_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXfixes_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXinerama_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xkbcomp_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXrandr_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLibglvnd_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mCairo_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXcursor_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_libXi_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mXorg_xkeyboard_config_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mHarfBuzz_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mxkbcommon_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mGLFW_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mlibass_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mQt5Base_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFFMPEG_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mFFMPEG\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mNLSolversBase\u001b[39m\n",
      "\u001b[91m  âœ— \u001b[39m\u001b[90mGR_jll\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39m\u001b[90mLineSearches\u001b[39m\n",
      "\u001b[32m  âœ“ \u001b[39mNLsolve\n",
      "\u001b[33m  âœ“ \u001b[39mDataFrames\n",
      "\u001b[32m  âœ“ \u001b[39mPlots\n",
      "  107 dependencies successfully precompiled in 79 seconds (64 already precompiled)\n",
      "  \u001b[33m13\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n",
      "  \u001b[91m1\u001b[39m dependency errored. To see a full report either run `import Pkg; Pkg.precompile()` or load the package\n"
     ]
    }
   ],
   "source": [
    "Pkg.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd041131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Kingston\\.julia\\environments\\v1.7\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Pandas\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"DataFrames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3a01de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”Œ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "â”” @ Base loading.jl:1423\n",
      "\u001b[33m\u001b[1mâ”Œ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule Tables with build ID 1498465727837401 is missing from the cache.\n",
      "\u001b[33m\u001b[1mâ”‚ \u001b[22m\u001b[39mThis may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1mâ”” \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1107\u001b[39m\n",
      "â”Œ Info: Skipping precompilation since __precompile__(false). Importing CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b].\n",
      "â”” @ Base loading.jl:1124\n",
      "â”Œ Info: Precompiling WeakRefStrings [ea10d353-3f73-51f8-a26c-33c1cb351aa5]\n",
      "â”” @ Base loading.jl:1423\n",
      "â”Œ Info: Precompiling FilePathsBase [48062228-2e41-5def-b9a4-89aafe57970f]\n",
      "â”” @ Base loading.jl:1423\n",
      "\u001b[33m\u001b[1mâ”Œ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule Compat with build ID 1498450496047801 is missing from the cache.\n",
      "\u001b[33m\u001b[1mâ”‚ \u001b[22m\u001b[39mThis may mean Compat [34da2185-b29b-5c13-b0c7-acf172513d20] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1mâ”” \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1107\u001b[39m\n",
      "â”Œ Info: Skipping precompilation since __precompile__(false). Importing FilePathsBase [48062228-2e41-5def-b9a4-89aafe57970f].\n",
      "â”” @ Base loading.jl:1124\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: InitError: MethodError: no method matching Parsers.Options(::Missing, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::Vector{String}, ::Vector{String}, ::Nothing, ::Bool, ::Bool, ::Nothing, ::Bool, ::Bool, ::Bool)\n\u001b[0mClosest candidates are:\n\u001b[0m  Parsers.Options(::Union{Missing, Nothing, Vector{String}}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Nothing, Char, UInt8, String}, ::Union{Char, UInt8}, ::Union{Nothing, Vector{String}}, ::Union{Nothing, Vector{String}}, ::Union{Nothing, String, Parsers.Format, Dates.DateFormat}, ::Any, ::Any, ::Any, ::Any, ::Any) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:81\n\u001b[0m  Parsers.Options(::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:59\n\u001b[0m  Parsers.Options(\u001b[91m::Vector{String}\u001b[39m, \u001b[91m::Union{Missing, Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}\u001b[39m, \u001b[91m::Bool\u001b[39m, \u001b[91m::Bool\u001b[39m, ::UInt8, ::UInt8, \u001b[91m::Bool\u001b[39m, ::UInt8, \u001b[91m::UInt8\u001b[39m, \u001b[91m::UInt8\u001b[39m, ::Union{Nothing, Tuple{Ptr{UInt8}, Int64}, UInt8}, \u001b[91m::UInt8\u001b[39m, \u001b[91m::Union{Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}\u001b[39m, ::Union{Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}, \u001b[91m::Union{Nothing, Parsers.Format}\u001b[39m, \u001b[91m::Union{Nothing, Tuple{Ptr{UInt8}, Int64}}\u001b[39m) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:59\nduring initialization of module CSV\nin expression starting at C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\CSV.jl:1",
     "output_type": "error",
     "traceback": [
      "LoadError: InitError: MethodError: no method matching Parsers.Options(::Missing, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::UInt8, ::Vector{String}, ::Vector{String}, ::Nothing, ::Bool, ::Bool, ::Nothing, ::Bool, ::Bool, ::Bool)\n\u001b[0mClosest candidates are:\n\u001b[0m  Parsers.Options(::Union{Missing, Nothing, Vector{String}}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Char, UInt8}, ::Union{Nothing, Char, UInt8, String}, ::Union{Char, UInt8}, ::Union{Nothing, Vector{String}}, ::Union{Nothing, Vector{String}}, ::Union{Nothing, String, Parsers.Format, Dates.DateFormat}, ::Any, ::Any, ::Any, ::Any, ::Any) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:81\n\u001b[0m  Parsers.Options(::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any, ::Any) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:59\n\u001b[0m  Parsers.Options(\u001b[91m::Vector{String}\u001b[39m, \u001b[91m::Union{Missing, Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}\u001b[39m, \u001b[91m::Bool\u001b[39m, \u001b[91m::Bool\u001b[39m, ::UInt8, ::UInt8, \u001b[91m::Bool\u001b[39m, ::UInt8, \u001b[91m::UInt8\u001b[39m, \u001b[91m::UInt8\u001b[39m, ::Union{Nothing, Tuple{Ptr{UInt8}, Int64}, UInt8}, \u001b[91m::UInt8\u001b[39m, \u001b[91m::Union{Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}\u001b[39m, ::Union{Nothing, Vector{Tuple{Ptr{UInt8}, Int64}}}, \u001b[91m::Union{Nothing, Parsers.Format}\u001b[39m, \u001b[91m::Union{Nothing, Tuple{Ptr{UInt8}, Int64}}\u001b[39m) at C:\\Users\\Kingston\\.julia\\packages\\Parsers\\a3jNK\\src\\Parsers.jl:59\nduring initialization of module CSV\nin expression starting at C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\CSV.jl:1",
      "",
      "Stacktrace:",
      "  [1] CSV.Context(source::CSV.Arg, header::CSV.Arg, normalizenames::CSV.Arg, datarow::CSV.Arg, skipto::CSV.Arg, footerskip::CSV.Arg, transpose::CSV.Arg, comment::CSV.Arg, ignoreemptyrows::CSV.Arg, ignoreemptylines::CSV.Arg, select::CSV.Arg, drop::CSV.Arg, limit::CSV.Arg, buffer_in_memory::CSV.Arg, threaded::CSV.Arg, ntasks::CSV.Arg, tasks::CSV.Arg, rows_to_check::CSV.Arg, lines_to_check::CSV.Arg, missingstrings::CSV.Arg, missingstring::CSV.Arg, delim::CSV.Arg, ignorerepeated::CSV.Arg, quoted::CSV.Arg, quotechar::CSV.Arg, openquotechar::CSV.Arg, closequotechar::CSV.Arg, escapechar::CSV.Arg, dateformat::CSV.Arg, dateformats::CSV.Arg, decimal::CSV.Arg, truestrings::CSV.Arg, falsestrings::CSV.Arg, stripwhitespace::CSV.Arg, type::CSV.Arg, types::CSV.Arg, typemap::CSV.Arg, pool::CSV.Arg, downcast::CSV.Arg, lazystrings::CSV.Arg, stringtype::CSV.Arg, strict::CSV.Arg, silencewarnings::CSV.Arg, maxwarnings::CSV.Arg, debug::CSV.Arg, parsingdebug::CSV.Arg, validate::CSV.Arg, streaming::CSV.Arg)",
      "    @ CSV C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\context.jl:366",
      "  [2] CSV.Context(source::IOBuffer; header::Int64, normalizenames::Bool, datarow::Int64, skipto::Int64, footerskip::Int64, transpose::Bool, comment::Nothing, ignoreemptyrows::Bool, ignoreemptylines::Nothing, select::Nothing, drop::Nothing, limit::Nothing, buffer_in_memory::Bool, threaded::Nothing, ntasks::Nothing, tasks::Nothing, rows_to_check::Int64, lines_to_check::Nothing, missingstrings::Vector{String}, missingstring::String, delim::Nothing, ignorerepeated::Bool, quoted::Bool, quotechar::Char, openquotechar::Nothing, closequotechar::Nothing, escapechar::Char, dateformat::Nothing, dateformats::Nothing, decimal::UInt8, truestrings::Vector{String}, falsestrings::Vector{String}, stripwhitespace::Bool, type::Nothing, types::Nothing, typemap::Dict{Type, Type}, pool::Tuple{Float64, Int64}, downcast::Bool, lazystrings::Bool, stringtype::Type{InlineStrings.InlineString}, strict::Bool, silencewarnings::Bool, maxwarnings::Int64, debug::Bool, parsingdebug::Bool, validate::Bool)",
      "    @ CSV C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\context.jl:178",
      "  [3] CSV.Context(source::IOBuffer)",
      "    @ CSV C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\context.jl:178",
      "  [4] __init__()",
      "    @ CSV C:\\Users\\Kingston\\.julia\\packages\\CSV\\jFiCn\\src\\CSV.jl:98",
      "  [5] include",
      "    @ .\\Base.jl:418 [inlined]",
      "  [6] _require(pkg::Base.PkgId)",
      "    @ Base .\\loading.jl:1149",
      "  [7] require(uuidkey::Base.PkgId)",
      "    @ Base .\\loading.jl:1013",
      "  [8] require(into::Module, mod::Symbol)",
      "    @ Base .\\loading.jl:997",
      "  [9] eval",
      "    @ .\\boot.jl:373 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "using Pandas\n",
    "using LinearAlgebra\n",
    "using DataFrames\n",
    "using CSV\n",
    "using Statistics\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3d701",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2de6fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: CSV not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: CSV not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:1",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "matrix_data_temp = CSV.File(\"median_housing_cost_data.tsv\") |> Tables.matrix\n",
    "matrix_data = matrix_data_temp[:,2:9];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_target_temp = CSV.File(\"housing_cost_targets.tsv\") |> Tables.matrix\n",
    "matrix_target = matrix_target_temp[:,2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split (test proportion of 20%) - test has 4128 samples, train has 16512 samples\n",
    "\n",
    "matrix_data_train = matrix_data[1:16512, :];\n",
    "matrix_data_test = matrix_data[16513:20640, :];\n",
    "\n",
    "matrix_target_train = matrix_target[1:16512, :];\n",
    "matrix_target_test = matrix_target[16513:20640, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data (min max normalize)\n",
    "#comment this out and run cell above if we wish to do it without normalizing\n",
    "\n",
    "for i in 1:size(matrix_data_train)[2]\n",
    "    \n",
    "    matrix_data_train[:,i] = (matrix_data_train[:,i] .- findmin(matrix_data_train[:,i])[1]) ./ (findmax(matrix_data_train[:,i])[1] - findmin(matrix_data_train[:,i])[1]);\n",
    "    matrix_data_test[:,i] = (matrix_data_test[:,i] .- findmin(matrix_data_test[:,i])[1]) ./ (findmax(matrix_data_test[:,i])[1] - findmin(matrix_data_test[:,i])[1]);\n",
    "\n",
    "end\n",
    "\n",
    "matrix_target_train = (matrix_target_train .- findmin(matrix_target_train)[1]) ./ (findmax(matrix_target_train)[1] - findmin(matrix_target_train)[1]);\n",
    "matrix_target_test = (matrix_target_test .- findmin(matrix_target_test)[1]) ./ (findmax(matrix_target_test)[1] - findmin(matrix_target_test)[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75200bd4",
   "metadata": {},
   "source": [
    "## Motivating factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02003089",
   "metadata": {},
   "source": [
    "Our motivating factor for our project is to address the lack of accuracy for real estate valuation, and to modernize the real estate sector. Due to the high fees of appraisers and the inconsistent valuations of homes, we would like to streamline and provide base accuracy for sellers to feel comfortable valuating their homes. \n",
    "\n",
    "The dataset we chose is restricted to homes in California, however, our models could be used to help Americans around the country to sell their homes on the market for fair and accurate prices.\n",
    "\n",
    "In our project, we implemented two linear regression methodologies (*Ridge Regression and Lasso Regression*) to create different ways of calculating the price of a home given certain characteristics. These two regressions can be used as Machine Learning algorithms to provide a great estimate on the cost of a home. Our goal for our algorithms is to determine which is better suited for our data. We will consider our model's accuracy, conditioning, algorithmic complexities, and number of floating point operations.\n",
    "\n",
    "Our ultimate goal from our machine learning models is to \"**maximize posterior probabilities**\" in order to produce the most likely prediction of the cost of a house in the real world. A posterior probability is a calculation of whether something is true in relation to previous observations.\n",
    "\n",
    "We would like to find the arguments ð‘¤ (linear regression parameters) which maximize the posterior probability of\n",
    "our outcomes. This formulation comes from a Bayesian approach called Maximum A Posteriori (MAP) that multiplies the\n",
    "probability of a data likelihood by its prior probability to get the posterior we are attempting to maximize. \n",
    "\n",
    "We have derived the MAP approaches into linear regression tasks with closed form solutions. The distributions of the\n",
    "data likelihoods and prior probabilities dictate the type of regression we will use. From our solutions, we show that data likelihood and prior probabilities both following Gaussian distributions result in ridge regression. We also show that data \n",
    "likelihood following a Gaussian distribution and prior probabilities following a Poisson distribution result in lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283209a",
   "metadata": {},
   "source": [
    "#### Dataset Features: \n",
    "- Median Income\n",
    "- House Age\n",
    "- Number of Rooms / House\n",
    "- Block Group Population\n",
    "- Number of Bedrooms\n",
    "- Occupancy\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n",
    "- Median House Price In Block *(Target)*\n",
    "\n",
    "##### Note: Our dataset is based off of US Census Data and is a dataset which is built into Python's popular SKLearn Library.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc8dd1",
   "metadata": {},
   "source": [
    "## Regression tasks derived from maximizing posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b41861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown derivations put here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80429285",
   "metadata": {},
   "source": [
    "## Solutions to regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "768dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown derivations put here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec865fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72868a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20571f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a2bf9dc",
   "metadata": {},
   "source": [
    "## Code solutions for regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64bab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_ridge (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear_regression_ridge(X,y,lambda)\n",
    "    s = size(X)[2]\n",
    "    Im = 1 * Matrix(I, s, s)\n",
    "    \n",
    "    w = inv(transpose(X)*X + lambda*Im) * transpose(X) * y\n",
    "    \n",
    "    pred = X * w\n",
    "    \n",
    "    err = y - pred\n",
    "    \n",
    "    return w    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f1f0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_lasso (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear_regression_lasso_GD(X, y, lambda)\n",
    "    \n",
    "    learning_rate = .025\n",
    "    iterations = 8000\n",
    "    l1_penalty = lambda\n",
    "    n = size(X)[2] # feature number\n",
    "    m = size(X)[1] # sample number\n",
    "    w = zeros(n) # shape of the params (feature #)\n",
    "    b = 0\n",
    "    \n",
    "    for i in 1:iterations        \n",
    "        y_pred = zeros(m)\n",
    "        for k in 1:m\n",
    "            y_pred[k] = dot(X[k,:], w) + b\n",
    "        end\n",
    "        \n",
    "        #calculate gradients\n",
    "        dW = zeros(n) # shape of the params (feature #)\n",
    "        for j in 1:n\n",
    "            if w[j] > 0  \n",
    "                dW[j] = ( -1 * (2 * (dot(X[:,j], y - y_pred) ) ) + l1_penalty) ./ m\n",
    "            else\n",
    "                dW[j] = ( -1 * (2 * (dot(X[:,j], y - y_pred) ) ) - l1_penalty) ./ m\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        db = - 2 * sum(y - y_pred) ./ m\n",
    "        \n",
    "        w = w - learning_rate*dW\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return w, b\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a01c4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linear_regression_test (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for performing linear regression on some data X with provided coefficients w\n",
    "function linear_regression_test(X,w)\n",
    "    return X * w                     # prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for performing linear regression on some data X with provided coefficients w for gradient descent regression\n",
    "function linear_regression_test_GD(X,w,b)\n",
    "    \n",
    "    m = size(X)[1]\n",
    "    pred = zeros(m)\n",
    "    for k in 1:m\n",
    "        pred[k] = dot(X[k,:], w) + b\n",
    "    end\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf36886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff64eaf9",
   "metadata": {},
   "source": [
    "## Analysis of algorithms: conditioning, complexities, and flop counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b9706",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d4c59",
   "metadata": {},
   "source": [
    "### Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1623a",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedd0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_TC (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge Regression Function Decomposed to show computations:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function ridge_TC(X,y,lambda)\n",
    "    s  = size(X)[2]                          # O(1)      [Accessing Size Variable]\n",
    "    Im = 1 * Matrix(I, s, s)                 # O(n^2)    [Constructing (n x n) I Matrix]\n",
    "    xT = transpose(X)                        # O(1)      [(m x n)] => (n x m)\n",
    "    xT_X = xT * X                            # O(m^2*n)  [(n x m) * (m x n)] => (n x n)\n",
    "    l_Im = lambda * Im                       # O(n^2)    [Scalar multiplication of (n x n) matrix]\n",
    "    xT_lambda = l_Im + xT_X                  # O(n^2)    [Scalar addition of (n x n) matrix]\n",
    "    inverse = inv(xT_lambda)                 # O(n^3)    [Inverse (Gauss Elimination) with matrix of size (n x n)]\n",
    "    w = inverse * xT                         # O(n^2*m)  [(n x n) * (n x m)] => (n x m)\n",
    "    w *= y                                   # O(m*n)    [(n x m) * (m x 1)] => (n x 1)\n",
    "    pred = X * w                             # O(m*n)    [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                           # O(m)      [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w, pred, err                      # O(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f50a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Complexities Added\n",
    "# O(n^3) + O(n^2*m) + O(m^2*n) + O(n^2) + O(n^2) + O(n^2) + O(m*n) + O(m*n) + O(m) + O(1) + O(1) + O(1)\n",
    "\n",
    "# Final Time Complexity: \n",
    "# O(n^3) + O(n^2*m) + O(m^2*n)\n",
    "\n",
    "# Note: If m >> n (Data points >> Features), time complexity can be reduced to O(m^2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d6c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Dimensions\n",
    "# --------------------------------\n",
    "# lambda = constant\n",
    "# (n x n) = xT_X, inverse, Im, l_Im xT_lambda\n",
    "# (n x m) = xT\n",
    "# (n x 1) = w\n",
    "# (m x n) = X\n",
    "# (m x 1) = y, pred, err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18de743",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb79532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - X: (m x n), y: (m x 1), lambda: constant, learning_rate: constant, iterations: constant\n",
    "# Variables  - m: rows, n: cols, i: iterations\n",
    "\n",
    "# Note: This is the \"expanded\" version of our function, allowing each instruction to have its own line.\n",
    "function timeC_lasso(X, y, lambda)\n",
    "    learning_rate = .025                    # O(1)   [Variable Assignment]\n",
    "    iterations = 8000                       # O(1)   [Variable Assignment]\n",
    "    l1_penalty = lambda                     # O(1)   [Variable Assignment]\n",
    "    n = size(X)[2]                          # O(1)   [Accessing Size Variable]\n",
    "    m = size(X)[1]                          # O(1)   [Accessing Size Variable]\n",
    "    w = zeros(n)                            # O(n)   [Creation of vector /w size n]\n",
    "    b = 0                                   # O(1)   [Variable Assignment]\n",
    "    \n",
    "    for i in 1:iterations                   # O(i)   [Loop]\n",
    "        y_pred = zeros(m)                   # O(m)     [Creation of vector /w size m]\n",
    "        y_res = y - y_pred                  # O(m)     [Vector Subtraction (m x 1) - (m x 1)]\n",
    "        \n",
    "        for k in 1:m                        # O(m)     [Loop]\n",
    "            dp = dot(X[k,:], w)             # O(n)         [(1 x n).(1 x n)]\n",
    "            dp += b                         # O(1)         [Scalar Addition]\n",
    "            y_pred[k] = dp                  # O(1)         [Value Assignment]\n",
    "        end\n",
    "        \n",
    "        # Calculate gradients\n",
    "        dW = zeros(n)                       # O(n)      [shape of the params (feature #)]\n",
    "        for j in 1:n                        # O(n)      [Loop]\n",
    "            dp_XY = dot(X[:,j], y_res)      # O(m)          [Dot Product (m x 1).(m x 1)]\n",
    "            dp_XY *= -2                     # O(1)          [Scalar Multiplication]\n",
    "            \n",
    "            if w[j] > 0                     # O(1)          [Accessing index] [IF/ELSE: Calculations / Iteration = 2*O(1)]\n",
    "                XY_pen = dp_XY + l1_penalty # O(1)          [Scalar Addition]\n",
    "                dW[j] = XY_pen / m          # O(1)          [Scalar Division]\n",
    "            else\n",
    "                XY_pen = dp_XY - l1_penalty # O(1)          [Scalar Subtraction]\n",
    "                dW[j] = XY_pen / m          # O(1)          [Scalar Subtraction]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        db = sum(y_res)                     # O(m)      [Summation of (m x 1)]\n",
    "        db *= -2                            # O(1)      [Scalar Multiplication]\n",
    "        db /= m                             # O(1)      [Scalar Division]\n",
    "        \n",
    "        lrdW = learning_rate*dW             # O(1)      [Scalar Multiplication]\n",
    "        w -= lrdW                           # O(1)      [Scalar Subtraction]\n",
    "        \n",
    "        lrdb = learning_rate*db             # O(1)      [Scalar Multiplication]\n",
    "        b -= lrdb                           # O(1)      [Scalar Subtraction]\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return w, b                             # O(1)   [Return Values]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6ce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Dimensions\n",
    "# ---------------------\n",
    "# Constant: learning_rate, iterations, l1_penalty, n, m, b, db, lrdW, lrdb, lambda\n",
    "# (n x 1) : w, dW\n",
    "# (m x 1) : y_pred, y_res, y\n",
    "# (m x n) : X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Time Complexity\n",
    "# (6*O(1) + O(n)) + i*(O(m) + O(m) + m(O(n) + O(1) + O(1)) + O(n) + n(O(m) + O(1) + O(1) + O(1) + O(1)) + O(m) + 7*O(1))\n",
    "# (6*O(1) + O(n)) + i*(5*O(m) + 2*O(m*n) + 5*O(n) + 7*O(1))\n",
    "# (2*O(m*n*i) + 5*O(m*i) + 5*O(n*i) + 7*O(i)) + (6*O(1) + O(n))\n",
    "# O(m*n*i) + O(m*i) + O(n*i) + O(i) + O(n) + O(1) \n",
    "\n",
    "# Final Time Complexity\n",
    "# O(m*n*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d101ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Complexity References\n",
    "# ----------------------------------------\n",
    "# Size(X)       : O(1)\n",
    "# References    : [https://stackoverflow.com/questions/21614298/what-is-the-runtime-of-array-length, https://blog.finxter.com/python-list-length-whats-the-runtime-complexity-of-len/, ]\n",
    "\n",
    "# Transpose(X)  : O(1)    \n",
    "# References    : [https://www.mathworks.com/matlabcentral/answers/495668-what-s-the-transpose-complexity-big-o-in-matlab, https://stackoverflow.com/questions/61157101/in-julia-transpose-operator]\n",
    "\n",
    "# Inverse(X)    : Worst Case-O(n^3) (Gauss Elimination), Best Case O(n^2.373)\n",
    "# References    : [https://stackoverflow.com/questions/54890422/inv-versus-on-julia]\n",
    "\n",
    "# Matrix *      : (m x n) * (n * p) => O(n*m*p), O(n^3)-O(n^2.72...)\n",
    "# References    : [https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra]\n",
    "\n",
    "# Matrix -      : O(m*n)\n",
    "# References    : [https://www.geeksforgeeks.org/different-operation-matrices/]\n",
    "\n",
    "# Matrix(I,s,s) : O(n^2)\n",
    "# References    : [https://stackoverflow.com/questions/282926/time-complexity-of-memory-allocation]\n",
    "\n",
    "# Zeros(n)      : O(n)\n",
    "# References    : [https://discourse.julialang.org/t/faster-zeros-with-calloc/69860/13, https://stackoverflow.com/questions/5640850/java-whats-the-big-o-time-of-declaring-an-array-of-size-n]\n",
    "\n",
    "# Dot(n, n)     : O(n)\n",
    "# References    : [https://helloacm.com/teaching-kids-programming-compute-the-dot-product-using-zip-function-in-python/#:~:text=The%20time%20complexity%20of%20dot,is%20O(1)%20constant.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94d5e0",
   "metadata": {},
   "source": [
    "### Space Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24dcb9",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a980c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ridge_SC (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual Ridge Algorithm\n",
    "function ridge_SC(X,y,lambda)                               # X: (m x n), Y: (m x 1), lambda: (constant)\n",
    "    n = size(X)[2]                                          # O(1)    1 variable\n",
    "    Im = 1 * Matrix(I, n, n)                                # O(n^2)  (n x n) matrix\n",
    "    w = inv(transpose(X)*X + lambda*Im) * transpose(X) * y  # O(n)    (n x 1) matrix\n",
    "                                                            # O(n^2)  (n x n) matrices [3 temporary matrices]\n",
    "                                                            # O(m*n)  (n x m) matrix   [1 temporary matrix]\n",
    "    pred = X * w                                            # O(m)    (m x 1) matrix\n",
    "    err = y - pred                                          # O(m)    (m x 1) matrix\n",
    "    \n",
    "    return w, pred, err                                     # O(1)    Return on Stack\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6321187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Space Complexity\n",
    "# O(n^2) + O(n^2) + O(n^2) + O(n^2) + O(n*m) + O(n) + O(m) + O(m) + O(1) + O(1)\n",
    "\n",
    "# Final Space Complexity\n",
    "# O(n^2) + O(n*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf95575",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - X: (m x n), y: (m x 1), lambda: constant, learning_rate: constant, iterations: constant\n",
    "# Variables  - m: rows, n: cols, i: iterations\n",
    "function linear_regression_lasso_GD(X, y, lambda)\n",
    "    learning_rate = .025                    # O(1)   [Variable Assignment]\n",
    "    iterations = 8000                       # O(1)   [Variable Assignment]\n",
    "    l1_penalty = lambda                     # O(1)   [Variable Assignment]\n",
    "    n = size(X)[2]                          # O(1)   [Accessing Size Variable]\n",
    "    m = size(X)[1]                          # O(1)   [Accessing Size Variable]\n",
    "    w = zeros(n)                            # O(n)   [Creation of vector /w size n]\n",
    "    b = 0                                   # O(1)   [Variable Assignment]\n",
    "    \n",
    "    for i in 1:iterations                   # O(i)   [Loop]\n",
    "        y_pred = zeros(m)                   # O(m)       [Creation of vector of size m]\n",
    "        y_res = y - y_pred                  # O(m)       [Creation of vector of size m]\n",
    "        \n",
    "        for k in 1:m                        # O(m)       [Loop]\n",
    "            y_pred[k] = dot(X[k,:], w) + b  # O(1)           [Storing Value]\n",
    "                                            # O(m)           [+ Temporary Dot Product Vector]\n",
    "                                            # O(1)           [+ Temporary Scalar Value]\n",
    "        end\n",
    "        \n",
    "        #calculate gradients\n",
    "        dW = zeros(n)                       # O(n)       [Creation of vector /w size n]\n",
    "        for j in 1:n                        # O(n)       [Loop]\n",
    "            dp_XY = -2 * dot(X[:,j], y_res) # O(m)          [Creation of vector size m]\n",
    "                                            # O(1)          [+ Temporary Scalar Value]\n",
    "            if w[j] > 0                     # ----          [Access Index] [IF/ELSE: Space / Iteration = 2*O(1)]\n",
    "                dW[j] = (dp_XY + l1_penalty) ./ m  # O(1)   [Variable Assignment]\n",
    "                                                   # O(1)   [+ Temporary Scalar Value]\n",
    "            else\n",
    "                dW[j] = (dp_XY - l1_penalty) ./ m  # O(1)   [Variable Assignment]\n",
    "                                                   # O(1)   [+ Temporary Scalar Value]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        db = - 2 * sum(y - y_pred) ./ m            # O(1)   [Variable Assignment]\n",
    "                                                   # O(1)   [+ Temporary Scalar Value] x2\n",
    "        \n",
    "        w = w - learning_rate*dW                   # O(1)   [Variable Assignment]\n",
    "                                                   # O(1)   [+ Temporary Scalar Value]\n",
    "        \n",
    "        b = b - learning_rate*db                   # O(1)   [Variable Assignment]\n",
    "                                                   # O(1)   [+ Temporary Scalar Value]\n",
    "    end\n",
    "    \n",
    "    return w, b                                    # O(n)   [Returning (n x 1)]\n",
    "                                                   # O(1)   [+ Returning scalar]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f130d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Space Complexity\n",
    "# Variables: m: rows, n: columns, i: iterations\n",
    "\n",
    "# (6*O(1) + O(n)) + i*(O(m) + O(m) + m*(O(1) + O(m) + O(1)) + O(n) + n*(O(m) + 3*O(1)) + O(n) + 7*O(1))\n",
    "# (6*O(1) + O(n)) + i*(O(m^2) + O(m*n) + 5*O(n) + 4*O(m) + 7*O(1))\n",
    "# (6*O(1) + O(n)) + i*(O(m^2) + O(m*n) + 5*O(n) + 4*O(m) + 7*O(1))\n",
    "# O(m^2*i) + O(m*n*i) + 5*O(n*i) + 4*O(m*i) + 7*O(i) + O(n) + 6*O(1)\n",
    "# O(m^2*i) + O(m*n*i) + O(n*i) + O(m*i) + O(i) + O(n) + O(1)\n",
    "\n",
    "# Final Space Complexity \n",
    "# O(m^2*i) + O(m*n*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space Complexity References\n",
    "# -------------------------------\n",
    "# [Logging Temporary Values] : [https://www.studytonight.com/data-structures/space-complexity-of-algorithms, https://www.geeksforgeeks.org/time-complexity-and-space-complexity/]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5da9ff",
   "metadata": {},
   "source": [
    "### Flop Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d0444",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression Function Decomposed:\n",
    "# Parameters- X: (m x n), y: (m x 1), lambda: constant\n",
    "function ridge_FC(X,y,lambda)\n",
    "    n  = size(X)[2]                          # 0\n",
    "    Im = 1 * Matrix(I, n, n)                 # n^2          [Constructing (n x n) I Matrix]\n",
    "    xT = transpose(X)                        # 0            [(m x n)] => (n x m)\n",
    "    xT_X = xT * X                            # m^2*n        [(n x m) * (m x n)] => (n x n)\n",
    "    l_Im = lambda * Im                       # n^2          [Scalar multiplication of (n x n) matrix]\n",
    "    xT_lambda = l_Im + xT_X                  # n^2          [Scalar addition of (n x n) matrix]\n",
    "    inverse = inv(xT_lambda)                 # n^3 (approx) [Gauss Elimination with matrix of size (n x n)]\n",
    "    w = inverse * xT                         # n^2*m        [(n x n) * (n x m)] => (n x m)\n",
    "    w *= y                                   # m*n          [(n x m) * (m x 1)] => (n x 1)\n",
    "    pred = X * w                             # m*n          [(m x n) * (n x 1)] => (m x 1)\n",
    "    err = y - pred                           # m            [(m x 1) - (m x 1)] => (m x 1)\n",
    "    \n",
    "    return w, pred, err                      # 0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b01f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count\n",
    "# n^2 + (m^2*n) + n^2 + n^2 + n^3 + n^2*m + m*n + m*n + m\n",
    "# Total Flop Count = n^3 + (3+m)n^2 + n*m^2 + 2m*n + m\n",
    "\n",
    "# Note: In the worst case, Gaussian Elimination for (n x n) will take ((5/6)n^3+(3/2)n^2-(7/6)n) floating point operations.\n",
    "# Worst Case Flops = (5/6)n^3 + ((9/2)+m)n^2 + n*m^2 + ((5/6)m)n + m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d629eb",
   "metadata": {},
   "source": [
    "#### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression Function Decomposed:\n",
    "# Parameters - X: (m x n), y: (m x 1), lambda: constant, learning_rate: constant, iterations: constant\n",
    "# Variables  - m: rows, n: cols, i: iterations\n",
    "\n",
    "# Note: This is the \"expanded\" version of our function, allowing each instruction to have its own line.\n",
    "function linear_regression_lasso_GD(X, y, lambda)\n",
    "    learning_rate = .025                    # 0\n",
    "    iterations = 8000                       # 0\n",
    "    l1_penalty = lambda                     # 0\n",
    "    n = size(X)[2]                          # 0\n",
    "    m = size(X)[1]                          # 0\n",
    "    w = zeros(n)                            # 0\n",
    "    b = 0                                   # 0\n",
    "    \n",
    "    for i in 1:iterations                   # i*   [Loop]\n",
    "        y_pred = zeros(m)                   # 0\n",
    "        y_res = y - y_pred                  # m         [Vector Subtraction (m x 1) - (m x 1)]\n",
    "        \n",
    "        for k in 1:m                        # m*        [Loop]\n",
    "            dp = dot(X[k,:], w)             # n            [(1 x n).(1 x n)]\n",
    "            dp += b                         # 1            [Scalar Addition]\n",
    "            y_pred[k] = dp                  # 0\n",
    "        end\n",
    "        \n",
    "        # Calculate gradients\n",
    "        dW = zeros(n)                       # 0\n",
    "        for j in 1:n                        # n*        [Loop]\n",
    "            dp_XY = dot(X[:,j], y_res)      # m             [Dot Product (m x 1).(m x 1)]\n",
    "            dp_XY *= -2                     # 1\n",
    "            \n",
    "            if w[j] > 0                     # -             [IF/ELSE: Flops / Iteration = 2]            \n",
    "                XY_pen = dp_XY + l1_penalty # 1                 [Scalar Addition]\n",
    "                dW[j] = XY_pen / m          # 1                 [Scalar Division]\n",
    "            else\n",
    "                XY_pen = dp_XY - l1_penalty # 1                 [Scalar Subtraction]\n",
    "                dW[j] = XY_pen / m          # 1                 [Scalar Subtraction]\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        db = sum(y_res)                     # m         [Summation of (m x 1)]\n",
    "        db *= -2                            # 1         [Scalar Multiplication]\n",
    "        db /= m                             # 1         [Scalar Division]\n",
    "        \n",
    "        lrdW = learning_rate*dW             # 1         [Scalar Multiplication]\n",
    "        w -= lrdW                           # 1         [Scalar Subtraction]\n",
    "        \n",
    "        lrdb = learning_rate*db             # 1         [Scalar Multiplication]\n",
    "        b -= lrdb                           # 1         [Scalar Subtraction]\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return w, b                             # O(1)   [Return Values]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58eb9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count:\n",
    "# Variables: m: rows, n: columns, i: iterations\n",
    "# i * (m + m*(n + 1) + n*(m + 1 + 1 + 1) + m + 1 + 1 + 1 + 1 + 1)\n",
    "\n",
    "# Total Flops = i * (2mn + 3m + 3n + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18960f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop Count References:\n",
    "# Flops For nxn Gaussian Elimination: http://web.mit.edu/18.06/www/Fall15/Matrices.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304cb4",
   "metadata": {},
   "source": [
    "## Perform regression tasks on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6586a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from dataframe into julia arrays and then call code solutions of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ad3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function r_squared(targets, predictions)\n",
    "    \n",
    "    mean_target = mean(targets)\n",
    "    \n",
    "    ssr = 0\n",
    "    sst = 0\n",
    "    \n",
    "    for i in 1:size(targets)[1]\n",
    "        \n",
    "        ssr += (targets[i] - predictions[i])^2\n",
    "        sst += (targets[i] - mean_target)^2\n",
    "        \n",
    "    end\n",
    "    \n",
    "    r_sq = 1 - (ssr/sst)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [.000001, .00001, .0001, .001, .01, .05, .1, .5, 1.5, 5, 10, 12, 15, 18, 20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_l1_params = zeros(8)\n",
    "optimal_l2_params = zeros(8)\n",
    "\n",
    "optimal_l1_r_sq = -1\n",
    "optimal_l2_r_sq = -1\n",
    "\n",
    "optimal_l1_lam = -1000\n",
    "optimal_l2_lam = -1000\n",
    "\n",
    "for it in lambdas\n",
    "    \n",
    "    parameters_l2 = linear_regression_ridge(matrix_data_train,matrix_target_train, it);\n",
    "    parameters_l1, b_l1 = linear_regression_lasso_GD(matrix_data_train,matrix_target_train, it);\n",
    "    \n",
    "    predictions_l2 = linear_regression_test(matrix_data_test, parameters_l2);\n",
    "    predictions_l1 = linear_regression_test_GD(matrix_data_test, parameters_l1, b_l1);\n",
    "    predictions_l1 = reshape(predictions_l1, length(predictions_l1), 1);\n",
    "    \n",
    "    r_sq_l2 = r_squared(sort(matrix_target_test, dims=1), sort(predictions_l2, dims=1));\n",
    "    r_sq_l1 = r_squared(sort(matrix_target_test, dims=1), sort(predictions_l1, dims=1));\n",
    "    \n",
    "    if r_sq_l2 > optimal_l2_r_sq\n",
    "        optimal_l2_r_sq = r_sq_l2\n",
    "        optimal_l2_params = parameters_l2\n",
    "        optimal_l2_lam = it\n",
    "    end\n",
    "    \n",
    "    if r_sq_l1 > optimal_l1_r_sq\n",
    "        optimal_l1_r_sq = r_sq_l1\n",
    "        optimal_l1_params = parameters_l1\n",
    "        optimal_l1_lam = it\n",
    "    end\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590263fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"The optimal lambda coefficient for l1 norm is:\", optimal_l1_lam)\n",
    "println(\"The optimal lambda coefficient for l2 norm is:\", optimal_l2_lam)\n",
    "println(\"The optimal r^2 value for l1 norm is:\", optimal_l1_r_sq)\n",
    "println(\"The optimal r^2 value for l2 norm is:\", optimal_l2_r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a3836",
   "metadata": {},
   "source": [
    "## Evaluation of algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r^2 and qq plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3ab80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard code in the optimal regularization coefficients to function\n",
    "parameters_l2 = linear_regression_ridge(matrix_data_train,matrix_target_train, 5);\n",
    "parameters_l1, b_l1 = linear_regression_lasso_GD(matrix_data_train,matrix_target_train, .00001);\n",
    "    \n",
    "predictions_l2 = linear_regression_test(matrix_data_test, parameters_l2);\n",
    "predictions_l1 = linear_regression_test_GD(matrix_data_test, parameters_l1, b_l1);\n",
    "predictions_l1 = reshape(predictions_l1, length(predictions_l1), 1);\n",
    "    \n",
    "sorted_test_targets = sort(matrix_target_test, dims=1)\n",
    "sorted_l2_preds = sort(predictions_l2, dims=1)\n",
    "sorted_l1_preds = sort(predictions_l1, dims=1)\n",
    "r_sq_l2 = r_squared(sorted_test_targets, sorted_l2_preds);\n",
    "r_sq_l1 = r_squared(sorted_test_targets, sorted_l1_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2dbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: someone please do this, my julia won't let me add the Plots package. Can someone graph the QQ plots here plz.\n",
    "# it should concist of 2 qq plots. each should have a x=y line. \n",
    "# first qq plot should have sorted_l1_preds vs sorted_test_targets\n",
    "# second qq plot should have sorted_l2_preds vs sorted_test_targets\n",
    "# then under each qq plot just print(r_sq_l#) then it is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70126167",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80172b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our results and conclusions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb8561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3bb0ae9",
   "metadata": {},
   "source": [
    "#### Complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea9d99",
   "metadata": {},
   "source": [
    "|   | Time Complexity | Space Complexity | Flop Count|\n",
    "| :----------- | :----------- | :----------- | :----------- |\n",
    "| **Ridge**  |$ O(n^3) + O(n^2*m) + O(m^2*n) $ | $O(n^2) +O(n*m)$ | $\\frac{5}{6}n^3 + (\\frac{9}{2}+m)n^2 + n*m^2 + (\\frac{5}{6}m)n + m$     |\n",
    "| **Lasso**  |$ O(m*n*i)$ |$  O(m^2*i) + O(m*n*i)     $|  $i*(2mn + 3m + 3n + 6)$   |\n",
    "| **Preferred Algorithm** |  Ridge *(When n < i < m)*   |  Ridge *(When m > n)*   | Lasso *(Unless: i > m > n)* | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b508b",
   "metadata": {},
   "source": [
    "Our results show that Ridge Regression is the better algorithm when it comes to Time Complexity, as even though inverting an (m x n matrix is O(n^3) when using Gauss Elimination, usually the number of rows (m) and iterations (i) of Lasso will be much larger than n. The only case where this would not be true is in the situation where the number of features is greater than the number of iterations for calculating the lasso algorithm, or the number of data entries. This situation would be very unusual, as m >> n and i >> n in most cases.\n",
    "\n",
    "When it comes to Space Complexity, Ridge regression uses less memory when the number of data points in the input matrix (m) is greater than the number of features. This should occur for most cases.\n",
    "\n",
    "In addition, Lasso and Ridge appear to be similar when it comes to floating point operations. If the number of rows is a larger value than n or i, Lasso will easily calculate less FLOPS due to not needing to use Gaussian Elimination to find the inverse of a large matrix. However, if the number of iterations (i) is larger than the number of rows (m), and the number of rows is greater than the number of features (n), the ridge regression will perform fewer floating point operations.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "**Ridge regression performs better than Lasso on time and space complexity in most cases.**\n",
    "\n",
    "However, Lasso regression has an advantage over ridge regression when it comes to flops (usually), due to matrix manipulation of Ridge regression taking more computational power. The only case where lasso has more floating point operations is when the number of iterations of the algorithm is greater than the number of data entries and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf3974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
